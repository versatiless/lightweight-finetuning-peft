{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "* PEFT technique: LoRA, QLoRA\n",
        "* Model: BERT\n",
        "* Evaluation approach: Accuracy and Macro-averaged F1 Score: Accuracy provides overall correctness, while Macro-F1 balances performance across all four classes equally.\n",
        "* Fine-tuning dataset: AG News Dataset: The AG’s News Topic Classification dataset consists of four categories from the original corpus: World, Sports, Business, and Sci/Tech. Each category includes 30,000 training samples and 1,900 test samples, resulting in a total of 120,000 training samples and 7,600 test samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f551c63a",
      "metadata": {
        "id": "f551c63a"
      },
      "outputs": [],
      "source": [
        "import os, random, numpy as np, torch, pandas as pd\n",
        "from collections import Counter\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e259f7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6ad01537d46c4998bd2b6d1986537026",
            "01de8f019f334c15aea856bb70a9d08d",
            "0b4e498424ac4386b43818de0a4427dc"
          ]
        },
        "id": "07e259f7",
        "outputId": "0b4b1b08-d7bc-4ed9-a1f8-da26951561af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ad01537d46c4998bd2b6d1986537026",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 18.6M/18.6M [00:00<00:00, 27.5MB/s]\n",
            "Downloading data: 100%|██████████| 1.23M/1.23M [00:00<00:00, 8.95MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01de8f019f334c15aea856bb70a9d08d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b4e498424ac4386b43818de0a4427dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 120000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 7600\n",
            "    })\n",
            "})\n",
            "Row 0:\n",
            "  text : Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "  label: 2\n",
            "Row 1:\n",
            "  text : Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "  label: 2\n",
            "Row 2:\n",
            "  text : Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "  label: 2\n"
          ]
        }
      ],
      "source": [
        "# Load the ag news dataset, and print the first three rows of them\n",
        "ds = load_dataset(\"ag_news\")\n",
        "print(\"Dataset loaded:\", ds)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Row {i}:\")\n",
        "    print(\"  text :\", ds[\"train\"][i][\"text\"])\n",
        "    print(\"  label:\", ds[\"train\"][i][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4935cb4d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d421b4b3c1ba497bbb1b6a7780291144",
            "3b0536bf44054dc1bf4790498c6a3355",
            "f472c3344481404588c1ef552a515fe9",
            "fc5b6bec291742a1aabb8f30486b4322",
            "85d996a36bef4304ac009a3f8d4368d7"
          ]
        },
        "id": "4935cb4d",
        "outputId": "735034f2-65f7-4cac-c63c-643a2629a7b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d421b4b3c1ba497bbb1b6a7780291144",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b0536bf44054dc1bf4790498c6a3355",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f472c3344481404588c1ef552a515fe9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc5b6bec291742a1aabb8f30486b4322",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85d996a36bef4304ac009a3f8d4368d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Load the BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    # Load the pretrained model for sequence classification\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=4)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model or tokenizer: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae99e862",
      "metadata": {
        "id": "ae99e862"
      },
      "outputs": [],
      "source": [
        "# Split 10% of the training set into a validation set, keep the original test set unchanged\n",
        "split = ds[\"train\"].train_test_split(\n",
        "    test_size=0.1,\n",
        "    seed=42,\n",
        "    stratify_by_column=\"label\"\n",
        ")\n",
        "dataset = DatasetDict({\n",
        "    \"train\": split[\"train\"],\n",
        "    \"validation\": split[\"test\"],\n",
        "    \"test\": ds[\"test\"],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ecf96c",
      "metadata": {
        "id": "53ecf96c",
        "outputId": "5dda97dd-83f1-44d6-a168-b7c157f47278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train counts    : {1: 27000, 3: 27000, 2: 27000, 0: 27000}\n",
            "Validation counts: {2: 3000, 0: 3000, 3: 3000, 1: 3000}\n",
            "Test counts     : {2: 1900, 3: 1900, 1: 1900, 0: 1900}\n"
          ]
        }
      ],
      "source": [
        "# Check class distribution in train, validation, and test sets to ensure they are balanced\n",
        "def count_labels(ds): return dict(Counter(ds[\"label\"]))\n",
        "\n",
        "print(\"Train counts    :\", count_labels(dataset[\"train\"]))\n",
        "print(\"Validation counts:\", count_labels(dataset[\"validation\"]))\n",
        "print(\"Test counts     :\", count_labels(dataset[\"test\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9534904e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "db41818c307645e4849f88564eb2fec6",
            "d09574c9c9a74cc4bcf2d497d9fa7e5e",
            "e28ccd00631243908958d8d795206404"
          ]
        },
        "id": "9534904e",
        "outputId": "e19ba356-718e-42c9-ce6d-5ed60895a1c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db41818c307645e4849f88564eb2fec6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/108000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d09574c9c9a74cc4bcf2d497d9fa7e5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e28ccd00631243908958d8d795206404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenize all datasets\n",
        "def tok_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "cols_to_remove = [c for c in dataset[\"train\"].column_names if c != \"label\"]\n",
        "ds_tok = dataset.map(tok_fn, batched=True, remove_columns=cols_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019b9f55",
      "metadata": {
        "id": "019b9f55"
      },
      "outputs": [],
      "source": [
        "# Define Evaluation Metrics: Accuracy and Macro-averaged F1 Score\n",
        "def accuracy_score_np(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()\n",
        "\n",
        "def f1_score_macro_np(y_true, y_pred, num_labels):\n",
        "    f1s = []\n",
        "    for label in range(num_labels):\n",
        "        tp = np.sum((y_pred == label) & (y_true == label))\n",
        "        fp = np.sum((y_pred == label) & (y_true != label))\n",
        "        fn = np.sum((y_pred != label) & (y_true == label))\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            f1s.append(0.0)\n",
        "        else:\n",
        "            f1s.append(2 * precision * recall / (precision + recall))\n",
        "    return float(np.mean(f1s))\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score_np(labels, preds)\n",
        "    macro_f1 = f1_score_macro_np(labels, preds, num_labels=4)\n",
        "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f6c600",
      "metadata": {
        "id": "72f6c600",
        "outputId": "476e5360-310e-407a-e8a2-04da8f32b84a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-finetune evaluation: {'eval_loss': 1.459642767906189, 'eval_accuracy': 0.25, 'eval_macro_f1': 0.10053574918182621, 'eval_runtime': 44.5747, 'eval_samples_per_second': 170.5, 'eval_steps_per_second': 5.339}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate BERT on the AG News test set before fine-tuning\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./pre_eval_bert_agnews\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    eval_dataset=ds_tok[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "pre_eval = trainer.evaluate()\n",
        "print(\"Pre-finetune evaluation:\", pre_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894046c0",
      "metadata": {
        "id": "894046c0",
        "outputId": "2e067067-cae0-42fe-ff54-3b160e2a5b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,684,936 || all params: 112,167,176 || trainable%: 2.393691359404466\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA for sequence classification\n",
        "lora_config = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47abf88",
      "metadata": {
        "id": "b47abf88",
        "outputId": "c377e521-5ac4-4939-b08e-1fd15b022dd0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/6750 05:57 < 34:17, 2.79 it/s, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>0.371929</td>\n",
              "      <td>0.878000</td>\n",
              "      <td>0.878003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.456900</td>\n",
              "      <td>0.399037</td>\n",
              "      <td>0.886750</td>\n",
              "      <td>0.886365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.439800</td>\n",
              "      <td>0.358828</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.885538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.054200</td>\n",
              "      <td>1.463574</td>\n",
              "      <td>0.250750</td>\n",
              "      <td>0.101556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.413800</td>\n",
              "      <td>1.388844</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.7304897518157959, metrics={'train_runtime': 357.3583, 'train_samples_per_second': 302.218, 'train_steps_per_second': 18.889, 'total_flos': 826308231785472.0, 'train_loss': 0.7304897518157959, 'epoch': 0.15})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fine-tune BERT with LoRA on the AG News training set\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/tmp/your_model_name\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    warmup_ratio=0.06,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tok[\"train\"],\n",
        "    eval_dataset=ds_tok[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ef1ff7",
      "metadata": {
        "id": "83ef1ff7",
        "outputId": "97230a5d-a0a7-474a-d744-73d276c7380e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved LoRA adapter to: /tmp/bert_lora_adapter\n"
          ]
        }
      ],
      "source": [
        "ADAPTER_DIR = \"/tmp/bert_lora_adapter\"\n",
        "lora_model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(\"Saved LoRA adapter to:\", ADAPTER_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863ec66e",
      "metadata": {
        "id": "863ec66e",
        "outputId": "1c683562-b954-47a4-8f81-a7e6c3ea0bf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PEFT (bert + LoRA) on AG News test: {'eval_loss': 0.40790843963623047, 'eval_accuracy': 0.8825, 'eval_macro_f1': 0.8820321664958857, 'eval_runtime': 26.0191, 'eval_samples_per_second': 292.093, 'eval_steps_per_second': 9.147}\n"
          ]
        }
      ],
      "source": [
        "# Run inference with the fine-tuned BERT + LoRA (PEFT) model on the AG News test set\n",
        "ADAPTER_DIR = \"/tmp/bert_lora_adapter\"\n",
        "BASE = \"bert-base-uncased\"\n",
        "NUM_LABELS = 4\n",
        "ID2LABEL = {0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"}\n",
        "LABEL2ID = {\"World\":0, \"Sports\":1, \"Business\":2, \"Sci/Tech\":3}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE, use_fast=True)\n",
        "model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
        "    ADAPTER_DIR,\n",
        "    num_labels=NUM_LABELS,\n",
        "    id2label=ID2LABEL,\n",
        "    label2id=LABEL2ID,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "args = TrainingArguments(output_dir=\"/tmp/eval_peft_bert\", per_device_eval_batch_size=32, report_to=[])\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    eval_dataset=ds_tok[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "lora_eval = trainer.evaluate()\n",
        "print(\"PEFT (bert + LoRA) on AG News test:\", lora_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e04b56",
      "metadata": {
        "id": "27e04b56",
        "outputId": "17c7461c-af63-4e2a-d1eb-f330b1221a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             Loss  Accuracy  Macro-F1\n",
            "Pre-finetune (baseline)  1.459643    0.2500  0.100536\n",
            "PEFT (BERT + LoRA)       0.407908    0.8825  0.882032\n",
            "Δ (PEFT - Base)         -1.051734    0.6325  0.781496\n"
          ]
        }
      ],
      "source": [
        "# Compare pre-finetune baseline and PEFT (BERT + LoRA) results\n",
        "def pick_metrics(m):\n",
        "    return {\n",
        "        \"Loss\":      float(m.get(\"eval_loss\", float(\"nan\"))),\n",
        "        \"Accuracy\":  float(m.get(\"eval_accuracy\", float(\"nan\"))),\n",
        "        \"Macro-F1\":  float(m.get(\"eval_macro_f1\", float(\"nan\"))),\n",
        "    }\n",
        "pre_m  = pick_metrics(pre_eval)\n",
        "peft_m = pick_metrics(lora_eval)\n",
        "df = pd.DataFrame(\n",
        "    [pre_m, peft_m],\n",
        "    index=[\"Pre-finetune (baseline)\", \"PEFT (BERT + LoRA)\"]\n",
        ")\n",
        "df.loc[\"Δ (PEFT - Base)\"] = df.iloc[1] - df.iloc[0]\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72d2f7a9",
      "metadata": {
        "id": "72d2f7a9"
      },
      "source": [
        "## QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a223314",
      "metadata": {
        "id": "2a223314"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "SAVE_ROOT = \"/tmp/peft_grid_bert_agnews\"\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "\n",
        "def pick_optim(model):\n",
        "    # Use paged_adamw_8bit for quantization to save GPU memory; otherwise use torch's AdamW\n",
        "    if getattr(model, \"is_loaded_in_4bit\", False) or getattr(model, \"is_loaded_in_8bit\", False):\n",
        "        return \"paged_adamw_8bit\"\n",
        "    return \"adamw_torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd180fcc",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c7e250db30a24378b26ccc7a57e140d0",
            "c69f98ccede646ab818cb0ca99fe5ef4",
            "368c33e9c5e6457e9d06c7963c7f4382"
          ]
        },
        "id": "fd180fcc",
        "outputId": "9ab3815e-6c1d-4b00-c59c-132f0e6b558f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7e250db30a24378b26ccc7a57e140d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/108000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c69f98ccede646ab818cb0ca99fe5ef4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "368c33e9c5e6457e9d06c7963c7f4382",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE, use_fast=True)\n",
        "\n",
        "def tok_fn(b):\n",
        "    return tokenizer(b[\"text\"], truncation=True, max_length=256)\n",
        "\n",
        "cols_to_remove = [c for c in dataset[\"train\"].column_names if c != \"label\"]\n",
        "ds_tok = dataset.map(tok_fn, batched=True, remove_columns=cols_to_remove)\n",
        "collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2657187d",
      "metadata": {
        "id": "2657187d"
      },
      "outputs": [],
      "source": [
        "# Model loader with fallbacks\n",
        "def load_model_with_fallback():\n",
        "    cfg = AutoConfig.from_pretrained(BASE, num_labels=4, id2label=ID2LABEL, label2id=LABEL2ID)\n",
        "    # Try 4-bit (QLoRA)\n",
        "    try:\n",
        "        bnb4 = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,   # T4: fp16 compute\n",
        "        )\n",
        "        print(\"[INFO] Try 4-bit NF4...\")\n",
        "        return AutoModelForSequenceClassification.from_pretrained(\n",
        "            BASE, config=cfg, quantization_config=bnb4, device_map={\"\":0}, low_cpu_mem_usage=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] 4-bit failed:\", repr(e))\n",
        "    # Fallback 8-bit\n",
        "    try:\n",
        "        bnb8 = BitsAndBytesConfig(load_in_8bit=True)\n",
        "        print(\"[INFO] Try 8-bit...\")\n",
        "        return AutoModelForSequenceClassification.from_pretrained(\n",
        "            BASE, config=cfg, quantization_config=bnb8, device_map={\"\":0}, low_cpu_mem_usage=True\n",
        "        )\n",
        "    except Exception as e2:\n",
        "        print(\"[WARN] 8-bit failed:\", repr(e2))\n",
        "    # Fallback full precision (GPU if available)\n",
        "    print(\"[INFO] Fallback to full-precision.\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE, config=cfg)\n",
        "    if use_cuda:\n",
        "        model = model.to(\"cuda\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88201312",
      "metadata": {
        "id": "88201312"
      },
      "outputs": [],
      "source": [
        "# PEFT grid (name, r, alpha, dropout, lr)\n",
        "PEFT_GRID = [\n",
        "    (\"lora_r8_a16_d05\",   8, 16, 0.05, 1e-3),\n",
        "    (\"lora_r16_a32_d10\", 16, 32, 0.10, 1e-3),\n",
        "    (\"lora_r32_a32_d10\", 32, 32, 0.10, 8e-4),\n",
        "]\n",
        "\n",
        "def count_params(m, trainable=False):\n",
        "    return int(sum(p.numel() for p in m.parameters() if (p.requires_grad if trainable else True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c8d50a",
      "metadata": {
        "id": "88c8d50a",
        "outputId": "d72202d0-f263-43e8-939d-586d87387801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== Experiment: lora_r8_a16_d05 (r=8, alpha=16, dropout=0.05, lr=0.001) ====\n",
            "[INFO] Try 4-bit NF4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,345,544 || all params: 110,827,784 || trainable%: 1.2140854499084814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13500/13500 20:57, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.358200</td>\n",
              "      <td>0.312480</td>\n",
              "      <td>0.893583</td>\n",
              "      <td>0.893373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.286800</td>\n",
              "      <td>0.280433</td>\n",
              "      <td>0.900167</td>\n",
              "      <td>0.900119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='613' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 01:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter to: /tmp/peft_grid_bert_agnews/lora_r8_a16_d05/adapter\n",
            "\n",
            "==== Experiment: lora_r16_a32_d10 (r=16, alpha=32, dropout=0.1, lr=0.001) ====\n",
            "[INFO] Try 4-bit NF4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,684,936 || all params: 112,167,176 || trainable%: 2.393691359404466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13500/13500 21:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.365300</td>\n",
              "      <td>0.305116</td>\n",
              "      <td>0.895417</td>\n",
              "      <td>0.895327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.290500</td>\n",
              "      <td>0.275519</td>\n",
              "      <td>0.899833</td>\n",
              "      <td>0.899802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='613' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 01:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter to: /tmp/peft_grid_bert_agnews/lora_r16_a32_d10/adapter\n",
            "\n",
            "==== Experiment: lora_r32_a32_d10 (r=32, alpha=32, dropout=0.1, lr=0.0008) ====\n",
            "[INFO] Try 4-bit NF4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 5,363,720 || all params: 114,845,960 || trainable%: 4.670360193776081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13500/13500 21:05, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.363300</td>\n",
              "      <td>0.303220</td>\n",
              "      <td>0.894750</td>\n",
              "      <td>0.894549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.295500</td>\n",
              "      <td>0.281204</td>\n",
              "      <td>0.898583</td>\n",
              "      <td>0.898540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='613' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 01:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapter to: /tmp/peft_grid_bert_agnews/lora_r32_a32_d10/adapter\n"
          ]
        }
      ],
      "source": [
        "# Run experiments\n",
        "results = []\n",
        "\n",
        "for name, r, alpha, dr, lr in PEFT_GRID:\n",
        "    print(f\"\\n==== Experiment: {name} (r={r}, alpha={alpha}, dropout={dr}, lr={lr}) ====\")\n",
        "    base_model = load_model_with_fallback()\n",
        "\n",
        "    if hasattr(base_model, \"gradient_checkpointing_enable\"):\n",
        "        base_model.gradient_checkpointing_enable()\n",
        "\n",
        "    # LoRA\n",
        "    lcfg = LoraConfig(\n",
        "        r=r, lora_alpha=alpha, lora_dropout=dr,\n",
        "        target_modules=[\"query\",\"key\",\"value\",\"dense\"],\n",
        "        bias=\"none\", task_type=TaskType.SEQ_CLS,\n",
        "    )\n",
        "    model = get_peft_model(base_model, lcfg)\n",
        "\n",
        "    # Force trainable parameters & classification head to FP32 (for stability)\n",
        "    for n, p in model.named_parameters():\n",
        "        if p.requires_grad and p.dtype != torch.float32:\n",
        "            p.data = p.data.float()\n",
        "    if hasattr(model, \"model\") and hasattr(model.model, \"classifier\"):\n",
        "        model.model.classifier = model.model.classifier.float()\n",
        "\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"{SAVE_ROOT}/{name}\",\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        learning_rate=lr,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        logging_steps=100,\n",
        "        save_total_limit=2,\n",
        "        report_to=[],\n",
        "        optim=pick_optim(base_model),\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=ds_tok[\"train\"],\n",
        "        eval_dataset=ds_tok[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluation：validation & test\n",
        "    val_metrics  = trainer.evaluate(ds_tok[\"validation\"])\n",
        "    test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
        "\n",
        "    # Save the LoRA adapter (lightweight)\n",
        "    adapter_dir = f\"{SAVE_ROOT}/{name}/adapter\"\n",
        "    model.save_pretrained(adapter_dir)\n",
        "    tokenizer.save_pretrained(adapter_dir)\n",
        "\n",
        "    row = {\n",
        "        \"run\": name, \"r\": r, \"alpha\": alpha, \"dropout\": dr, \"lr\": lr,\n",
        "        \"val_acc\":  float(val_metrics.get(\"eval_accuracy\", np.nan)),\n",
        "        \"val_f1\":   float(val_metrics.get(\"eval_macro_f1\", np.nan)),\n",
        "        \"test_acc\": float(test_metrics.get(\"eval_accuracy\", np.nan)),\n",
        "        \"test_f1\":  float(test_metrics.get(\"eval_macro_f1\", np.nan)),\n",
        "        \"trainable_params\": count_params(model, trainable=True),\n",
        "        \"total_params\":     count_params(model, trainable=False),\n",
        "        \"adapter_dir\": adapter_dir,\n",
        "    }\n",
        "    results.append(row)\n",
        "    print(\"Saved adapter to:\", adapter_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d83b8b3",
      "metadata": {
        "id": "4d83b8b3",
        "outputId": "4509314a-5ef6-499b-d2d3-7af5dd1a4091"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>r</th>\n",
              "      <th>alpha</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>test_f1</th>\n",
              "      <th>trainable_params</th>\n",
              "      <th>adapter_dir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lora_r8_a16_d05</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.900167</td>\n",
              "      <td>0.900119</td>\n",
              "      <td>0.898026</td>\n",
              "      <td>0.897948</td>\n",
              "      <td>1345544</td>\n",
              "      <td>/tmp/peft_grid_bert_agnews/lora_r8_a16_d05/ada...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lora_r16_a32_d10</td>\n",
              "      <td>16</td>\n",
              "      <td>32</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.899833</td>\n",
              "      <td>0.899802</td>\n",
              "      <td>0.898289</td>\n",
              "      <td>0.898238</td>\n",
              "      <td>2684936</td>\n",
              "      <td>/tmp/peft_grid_bert_agnews/lora_r16_a32_d10/ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lora_r32_a32_d10</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.898583</td>\n",
              "      <td>0.898540</td>\n",
              "      <td>0.897500</td>\n",
              "      <td>0.897415</td>\n",
              "      <td>5363720</td>\n",
              "      <td>/tmp/peft_grid_bert_agnews/lora_r32_a32_d10/ad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                run   r  alpha  dropout      lr   val_acc    val_f1  test_acc  \\\n",
              "0   lora_r8_a16_d05   8     16     0.05  0.0010  0.900167  0.900119  0.898026   \n",
              "1  lora_r16_a32_d10  16     32     0.10  0.0010  0.899833  0.899802  0.898289   \n",
              "2  lora_r32_a32_d10  32     32     0.10  0.0008  0.898583  0.898540  0.897500   \n",
              "\n",
              "    test_f1  trainable_params  \\\n",
              "0  0.897948           1345544   \n",
              "1  0.898238           2684936   \n",
              "2  0.897415           5363720   \n",
              "\n",
              "                                         adapter_dir  \n",
              "0  /tmp/peft_grid_bert_agnews/lora_r8_a16_d05/ada...  \n",
              "1  /tmp/peft_grid_bert_agnews/lora_r16_a32_d10/ad...  \n",
              "2  /tmp/peft_grid_bert_agnews/lora_r32_a32_d10/ad...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best by validation macro-F1 -> lora_r8_a16_d05 \n",
            "Adapter path: /tmp/peft_grid_bert_agnews/lora_r8_a16_d05/adapter\n"
          ]
        }
      ],
      "source": [
        "# Summary table\n",
        "df = pd.DataFrame(results).sort_values([\"val_f1\",\"test_f1\"], ascending=False).reset_index(drop=True)\n",
        "display(df[[\"run\",\"r\",\"alpha\",\"dropout\",\"lr\",\"val_acc\",\"val_f1\",\"test_acc\",\"test_f1\",\"trainable_params\",\"adapter_dir\"]])\n",
        "print(\"\\nBest by validation macro-F1 ->\", df.iloc[0][\"run\"], \"\\nAdapter path:\", df.iloc[0][\"adapter_dir\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88046b0a",
      "metadata": {
        "id": "88046b0a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}